# HUGO - Helpful Universal Guide & Organizer

**Voice-First Personal Assistant Embodied in a Reachy Mini Robot**

---

## Project Overview

HUGO is a voice-first personal assistant that lives inside a Reachy Mini desktop robot. It uses multi-agent AI orchestration to help manage email, calendar, project management, meeting transcripts, and general conversation ‚Äî all controlled through natural voice commands.

**The Idea:** Instead of switching between a dozen apps and tabs throughout the day, just talk to a robot on your desk. HUGO listens, understands what you need, routes your request to the right specialist agent, and responds with both voice and physical robot expressions.

**The Interaction Loop:** Voice Input ‚Üí Voice Activity Detection ‚Üí Speech-to-Text ‚Üí Semantic Intent Routing ‚Üí Specialist AI Agent ‚Üí Text-to-Speech ‚Üí Robot Expression & Animation

## Technical Architecture

### 7 Specialist AI Agents (CrewAI)

Each user utterance gets classified by a semantic router and dispatched to the right agent:

- **Email Agent** ‚Äî Read, search, summarize, draft, and send emails via Microsoft Graph
- **Calendar Agent** ‚Äî View schedule, check availability, create events via Microsoft Graph
- **Linear Agent** ‚Äî View assigned issues, create tickets, update status via Linear GraphQL API
- **Fireflies Agent** ‚Äî Search meeting transcripts, extract action items and decisions via Fireflies.ai
- **Vision Agent** ‚Äî Analyze camera feed, describe scenes, read text using Qwen3-VL
- **General Agent** ‚Äî Conversation, general knowledge, small talk
- **Orchestrator** ‚Äî Routes and synthesizes across all agents for complex multi-domain requests

### Local-First Voice Pipeline (Apple Silicon / MLX)

~90% of all inference runs entirely on-device:

- **Voice Activity Detection**: Silero VAD via PyTorch
- **Speech-to-Text**: Whisper V3 Turbo via mlx-audio
- **Text-to-Speech**: Kokoro-82M via mlx-audio
- **Voice Pipeline Framework**: Pipecat

### LLM Inference with Fallback Chain

1. **Primary (local)**: Qwen3-32B via mlx-lm (4-bit quantized on Apple Silicon)
2. **Cloud fallback tier 1**: Gemini 2.5 Flash
3. **Cloud fallback tier 2**: Claude Sonnet 4.5 (for the hardest tasks)

### Sub-Millisecond Intent Routing

Uses nomic-embed-text V2 (Mixture of Experts) semantic embeddings via the semantic-router library to classify user utterances into 7 categories before any LLM call ‚Äî making routing nearly instantaneous.

### MCP (Model Context Protocol) Servers

Each external service is implemented as a standalone FastMCP server:

- **Microsoft Graph** ‚Äî Email, calendar, files via Azure Identity + msgraph-sdk
- **Linear** ‚Äî Issue and project management via Linear GraphQL API
- **Fireflies.ai** ‚Äî Meeting transcript search via Fireflies GraphQL API

### Robot Control

- **Reachy Mini SDK** ‚Äî Head movement, antenna-based emotional expressions (happy, sad, thinking, surprised, wiggle), body rotation, camera capture
- **Simulation mode** (`--sim`) for development without hardware
- **Text-only mode** (`--no-voice`) for testing agent logic

## Technology Stack

- **Language**: Python 3.12 with strict mypy typing
- **Agent Framework**: CrewAI with YAML-configured agents and tasks
- **Voice**: Silero VAD, Whisper V3 Turbo, Kokoro-82M, Pipecat (all via MLX)
- **LLMs**: Qwen3-32B (local), Qwen3-VL 4B (vision), Gemini 2.5 Flash, Claude Sonnet 4.5
- **Semantic Routing**: nomic-embed-text V2 via semantic-router
- **MCP Servers**: FastMCP for Microsoft Graph, Linear, Fireflies.ai
- **Robot**: Reachy Mini SDK (Pollen Robotics)
- **Data Modeling**: Pydantic v2 for structured outputs
- **Package Management**: uv with hatchling build backend
- **CI/CD**: GitHub Actions, Ruff, mypy (strict), pytest, Bandit, Gitleaks, commitlint

## Key Features

- **Voice-First Interaction** ‚Äî Speak naturally; the robot listens, processes, and responds with voice and physical expressions
- **Privacy-First** ‚Äî 90% of inference runs locally on Apple Silicon, no data sent to cloud unless the local model can't handle it
- **Approval Gates** ‚Äî Safety mechanism requiring explicit confirmation before sending emails, creating issues, or scheduling events
- **Physical Embodiment** ‚Äî Robot antenna emotions and head movements give the assistant personality and presence
- **Simulation Mode** ‚Äî Full development experience without robot hardware

---

**Links:**

- üìÅ [GitHub Repository](https://github.com/EduardKakosyan/hugo)
